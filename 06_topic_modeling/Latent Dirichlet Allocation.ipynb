{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64e711c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d074a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-b4124dfb5e6a>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755f0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# 불용어를 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b13da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased, disagree, st...\n",
       "1    [yeah, expect, people, read, actually, accept,...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4    [well, change, scoring, playoff, pool, unfortu...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d4b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f33b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.021*\"azerbaijan\" + 0.015*\"karina\" + 0.012*\"karabakh\" + 0.012*\"picture\"')\n",
      "(1, '0.021*\"health\" + 0.015*\"medical\" + 0.013*\"guns\" + 0.011*\"disease\"')\n",
      "(2, '0.025*\"armenian\" + 0.024*\"jews\" + 0.021*\"turkish\" + 0.019*\"armenians\"')\n",
      "(3, '0.014*\"nist\" + 0.011*\"germany\" + 0.010*\"ncsl\" + 0.008*\"decenso\"')\n",
      "(4, '0.013*\"jesus\" + 0.008*\"christian\" + 0.008*\"bible\" + 0.007*\"believe\"')\n",
      "(5, '0.013*\"thanks\" + 0.011*\"would\" + 0.011*\"know\" + 0.010*\"drive\"')\n",
      "(6, '0.021*\"file\" + 0.013*\"program\" + 0.010*\"files\" + 0.009*\"available\"')\n",
      "(7, '0.016*\"encryption\" + 0.014*\"chip\" + 0.012*\"keys\" + 0.012*\"clipper\"')\n",
      "(8, '0.016*\"cross\" + 0.013*\"phillies\" + 0.008*\"vpic\" + 0.007*\"steel\"')\n",
      "(9, '0.016*\"radar\" + 0.010*\"detector\" + 0.010*\"maine\" + 0.008*\"reds\"')\n",
      "(10, '0.025*\"water\" + 0.016*\"cover\" + 0.014*\"neutral\" + 0.010*\"copies\"')\n",
      "(11, '0.015*\"play\" + 0.013*\"period\" + 0.013*\"hockey\" + 0.012*\"game\"')\n",
      "(12, '0.014*\"space\" + 0.007*\"information\" + 0.006*\"data\" + 0.006*\"university\"')\n",
      "(13, '0.020*\"said\" + 0.011*\"went\" + 0.008*\"know\" + 0.008*\"left\"')\n",
      "(14, '0.019*\"year\" + 0.011*\"last\" + 0.010*\"game\" + 0.010*\"team\"')\n",
      "(15, '0.012*\"government\" + 0.009*\"state\" + 0.009*\"israel\" + 0.009*\"president\"')\n",
      "(16, '0.018*\"would\" + 0.012*\"people\" + 0.010*\"think\" + 0.010*\"like\"')\n",
      "(17, '0.021*\"contest\" + 0.017*\"filename\" + 0.014*\"banks\" + 0.013*\"gordon\"')\n",
      "(18, '0.013*\"scsi\" + 0.008*\"power\" + 0.007*\"bike\" + 0.006*\"speed\"')\n",
      "(19, '0.013*\"cubs\" + 0.012*\"mask\" + 0.011*\"hanging\" + 0.011*\"pixmap\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 20 #20개의 토픽, k=20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4391c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.021*\"azerbaijan\" + 0.015*\"karina\" + 0.012*\"karabakh\" + 0.012*\"picture\" + 0.012*\"openwindows\" + 0.011*\"azeri\" + 0.010*\"azerbaijanis\" + 0.010*\"sleeve\" + 0.009*\"azeris\" + 0.009*\"bodies\"'), (1, '0.021*\"health\" + 0.015*\"medical\" + 0.013*\"guns\" + 0.011*\"disease\" + 0.010*\"study\" + 0.009*\"among\" + 0.009*\"patients\" + 0.009*\"rate\" + 0.009*\"drug\" + 0.009*\"control\"'), (2, '0.025*\"armenian\" + 0.024*\"jews\" + 0.021*\"turkish\" + 0.019*\"armenians\" + 0.013*\"turkey\" + 0.013*\"jewish\" + 0.011*\"greek\" + 0.010*\"turks\" + 0.009*\"muslim\" + 0.009*\"genocide\"'), (3, '0.014*\"nist\" + 0.011*\"germany\" + 0.010*\"ncsl\" + 0.008*\"decenso\" + 0.006*\"finland\" + 0.006*\"sword\" + 0.006*\"creed\" + 0.006*\"april\" + 0.006*\"promo\" + 0.005*\"dean\"'), (4, '0.013*\"jesus\" + 0.008*\"christian\" + 0.008*\"bible\" + 0.007*\"believe\" + 0.007*\"church\" + 0.006*\"christians\" + 0.005*\"true\" + 0.005*\"faith\" + 0.005*\"christ\" + 0.005*\"religion\"'), (5, '0.013*\"thanks\" + 0.011*\"would\" + 0.011*\"know\" + 0.010*\"drive\" + 0.010*\"anyone\" + 0.010*\"please\" + 0.010*\"card\" + 0.009*\"system\" + 0.009*\"like\" + 0.009*\"disk\"'), (6, '0.021*\"file\" + 0.013*\"program\" + 0.010*\"files\" + 0.009*\"available\" + 0.009*\"window\" + 0.009*\"windows\" + 0.008*\"version\" + 0.008*\"output\" + 0.008*\"entry\" + 0.007*\"server\"'), (7, '0.016*\"encryption\" + 0.014*\"chip\" + 0.012*\"keys\" + 0.012*\"clipper\" + 0.012*\"security\" + 0.011*\"public\" + 0.010*\"privacy\" + 0.010*\"government\" + 0.009*\"algorithm\" + 0.009*\"system\"'), (8, '0.016*\"cross\" + 0.013*\"phillies\" + 0.008*\"vpic\" + 0.007*\"steel\" + 0.007*\"ghetto\" + 0.006*\"conception\" + 0.005*\"adjusted\" + 0.005*\"caches\" + 0.005*\"warsaw\" + 0.005*\"labeled\"'), (9, '0.016*\"radar\" + 0.010*\"detector\" + 0.010*\"maine\" + 0.008*\"reds\" + 0.007*\"quicktime\" + 0.007*\"init\" + 0.007*\"clip\" + 0.006*\"detectors\" + 0.006*\"corn\" + 0.006*\"indians\"'), (10, '0.025*\"water\" + 0.016*\"cover\" + 0.014*\"neutral\" + 0.010*\"copies\" + 0.009*\"green\" + 0.008*\"appears\" + 0.007*\"issue\" + 0.006*\"conductor\" + 0.006*\"annual\" + 0.006*\"plants\"'), (11, '0.015*\"play\" + 0.013*\"period\" + 0.013*\"hockey\" + 0.012*\"game\" + 0.011*\"team\" + 0.010*\"season\" + 0.008*\"power\" + 0.008*\"games\" + 0.008*\"pittsburgh\" + 0.008*\"detroit\"'), (12, '0.014*\"space\" + 0.007*\"information\" + 0.006*\"data\" + 0.006*\"university\" + 0.005*\"list\" + 0.005*\"nasa\" + 0.005*\"mail\" + 0.005*\"also\" + 0.005*\"send\" + 0.004*\"research\"'), (13, '0.020*\"said\" + 0.011*\"went\" + 0.008*\"know\" + 0.008*\"left\" + 0.008*\"people\" + 0.007*\"back\" + 0.007*\"told\" + 0.007*\"came\" + 0.007*\"took\" + 0.007*\"home\"'), (14, '0.019*\"year\" + 0.011*\"last\" + 0.010*\"game\" + 0.010*\"team\" + 0.010*\"good\" + 0.007*\"games\" + 0.006*\"first\" + 0.006*\"well\" + 0.006*\"players\" + 0.006*\"would\"'), (15, '0.012*\"government\" + 0.009*\"state\" + 0.009*\"israel\" + 0.009*\"president\" + 0.008*\"people\" + 0.007*\"states\" + 0.006*\"rights\" + 0.006*\"israeli\" + 0.005*\"right\" + 0.005*\"american\"'), (16, '0.018*\"would\" + 0.012*\"people\" + 0.010*\"think\" + 0.010*\"like\" + 0.008*\"know\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"could\" + 0.006*\"well\" + 0.006*\"much\"'), (17, '0.021*\"contest\" + 0.017*\"filename\" + 0.014*\"banks\" + 0.013*\"gordon\" + 0.013*\"pitt\" + 0.012*\"soon\" + 0.011*\"surrender\" + 0.011*\"guidelines\" + 0.010*\"judges\" + 0.010*\"skepticism\"'), (18, '0.013*\"scsi\" + 0.008*\"power\" + 0.007*\"bike\" + 0.006*\"speed\" + 0.006*\"used\" + 0.005*\"engine\" + 0.005*\"drive\" + 0.005*\"printf\" + 0.005*\"cars\" + 0.005*\"char\"'), (19, '0.013*\"cubs\" + 0.012*\"mask\" + 0.011*\"hanging\" + 0.011*\"pixmap\" + 0.009*\"chinese\" + 0.008*\"bandwidth\" + 0.007*\"pointer\" + 0.007*\"food\" + 0.007*\"edge\" + 0.006*\"mydisplay\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d444ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문서의 topic 비율은 [(2, 0.3024449), (3, 0.017191047), (10, 0.017595202), (15, 0.16696566), (16, 0.4834907)]\n",
      "1 번째 문서의 topic 비율은 [(4, 0.21247756), (10, 0.027626049), (11, 0.027286127), (12, 0.068444334), (14, 0.04111916), (16, 0.6045902)]\n",
      "2 번째 문서의 topic 비율은 [(2, 0.02164273), (15, 0.28209135), (16, 0.6825209)]\n",
      "3 번째 문서의 topic 비율은 [(4, 0.18545993), (5, 0.07903979), (7, 0.31767324), (13, 0.03735201), (14, 0.030705806), (16, 0.33881307)]\n",
      "4 번째 문서의 topic 비율은 [(6, 0.070941694), (11, 0.5246291), (16, 0.37293595)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/formegusto/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i==5:\n",
    "        break\n",
    "    print(i,'번째 문서의 topic 비율은',topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432e7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/formegusto/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
    "\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "311b746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/formegusto/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중이 높은 토픽</th>\n",
       "      <th>가장 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>[(2, 0.30246454), (3, 0.01719105), (10, 0.0175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.6046</td>\n",
       "      <td>[(4, 0.2124694), (10, 0.027626049), (11, 0.027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>[(2, 0.021642968), (15, 0.28210944), (16, 0.68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>[(4, 0.1854665), (5, 0.079038136), (7, 0.31767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5247</td>\n",
       "      <td>[(6, 0.07077584), (11, 0.5246811), (16, 0.3730...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.7216</td>\n",
       "      <td>[(2, 0.054139797), (4, 0.12025933), (10, 0.067...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.3692</td>\n",
       "      <td>[(4, 0.055864904), (5, 0.36919996), (6, 0.0912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>[(15, 0.38254887), (16, 0.5690251), (18, 0.034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.4403</td>\n",
       "      <td>[(1, 0.031395994), (6, 0.1339345), (14, 0.0361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.3745</td>\n",
       "      <td>[(2, 0.019394146), (5, 0.15080054), (7, 0.0597...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   문서 번호  가장 비중이 높은 토픽  가장 높은 토픽의 비중  \\\n",
       "0      0          16.0        0.4835   \n",
       "1      1          16.0        0.6046   \n",
       "2      2          16.0        0.6825   \n",
       "3      3          16.0        0.3388   \n",
       "4      4          11.0        0.5247   \n",
       "5      5          16.0        0.7216   \n",
       "6      6           5.0        0.3692   \n",
       "7      7          16.0        0.5690   \n",
       "8      8          16.0        0.4403   \n",
       "9      9          16.0        0.3745   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(2, 0.30246454), (3, 0.01719105), (10, 0.0175...  \n",
       "1  [(4, 0.2124694), (10, 0.027626049), (11, 0.027...  \n",
       "2  [(2, 0.021642968), (15, 0.28210944), (16, 0.68...  \n",
       "3  [(4, 0.1854665), (5, 0.079038136), (7, 0.31767...  \n",
       "4  [(6, 0.07077584), (11, 0.5246811), (16, 0.3730...  \n",
       "5  [(2, 0.054139797), (4, 0.12025933), (10, 0.067...  \n",
       "6  [(4, 0.055864904), (5, 0.36919996), (6, 0.0912...  \n",
       "7  [(15, 0.38254887), (16, 0.5690251), (18, 0.034...  \n",
       "8  [(1, 0.031395994), (6, 0.1339345), (14, 0.0361...  \n",
       "9  [(2, 0.019394146), (5, 0.15080054), (7, 0.0597...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
    "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topictable[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31363ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
