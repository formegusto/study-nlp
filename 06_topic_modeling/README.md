# 토픽 모델링(Topic Modeling)

- 토픽(Topic)은 한국어로는 주제라고 한다. 토픽 모델링이란 기계 학습 및 자연어 처리 분야에서 토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나이다.
- 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법이다.

# 1. 잠재 의미 분석 (Latent Semantic Analysis, LSA)

- LSA는 정확히는 토픽 모델링을 위해 최적화 된 알고리즘은 아니지만, 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘이다.
- BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에, 단어의 의미를 고려하지 못한다는 단점이 있었다. 이를 위한 대안으로 DTM의 잠재된(Latent) 의미를 이끌어내는 방법으로 잠재 의미 분석(LSA)이라는 방법이 있다.
- 잠재 의미 분석 (Latent Semantic Indexing, LSI)이라고 부르기도 한다.
- 이 방법을 이해하기 위해서는 선형대수학의 특이값 분해(Singular Value Decomposition, SVD)를 이해할 필요가 있다.

## 1. 특이값 분해 (Singular Value Decomposition, SVD)

- SVD란 A가 m x n 행렬일 때, 3개의 행렬의 곱으로 분해하는 것을 말한다.

$$A = U\Sigma V^T$$

- 여기서 각 3개의 행렬은 다음과 같은 조건을 만족한다.
  - U : m \* m 직교행렬
  - V : n \* n 직교행렬
  - $\Sigma$ : m \* n 직사각 대각행렬
- 여기서 직교행렬이란 자신과 자신의 전치행렬의 곱 또는 이를 반대로 곱한 결과가 단위행렬이 되는 행렬을 말한다.
- 또한 대각행렬이란 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 의미한다.
- 이때 SVD로 나온 대각 행렬의 대각 원소의 값을 행렬 A의 특이값(singular value)라고 한다.

### 1. 전치 행렬 (Transposed Matrix)

- 원래의 행렬에서 행과 열을 바꾼 행렬이다.
- 즉, 주대각선을 축으로 반사 대칭을 하여 얻는 행렬이다.
- 기호는 기존 행렬 표현의 우측 위에 T를 붙인다.

### 2. 단위 행렬 (Identity Matrix)

- 단위행렬은 주대각선의 원소가 모두 1이며 나머지 원소는 모두 0인 정사각 행렬을 말한다.
- 보통 줄여서 대문자 I로 표현하기도 한다.

### 3. 역행렬 (Inverse Matrix)

- 행렬 A와 어떤 행렬을 곱했을 때, 결과로서 단위 행렬이 나온다면 이때의 어떤 행렬을 A의 역행렬이라고 한다.

### 4. 직교 행렬 (Orthogonal Matrix)

- 실수 n _ n 행렬 A에 대해서 $A _ A^T=I$ 를 만족하면서 $A^T * A = I$을 만족하는 행렬 A를 직교 행렬이라고 한다.
- 그런데 역행렬의 정의를 다시 생각해보면, 결국 직교 행렬은 $A^-1=A^T$를 만족한다.

### 5. 대각 행렬 (Diagonal Matrix)

- 대각행렬은 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 말한다.
- 만약 행의 크기가 열의 크기보다 크다면 m x m의 행렬만 대각선에 포함한다.
- 반대의 경우도 마찬가지
- 대각 행렬 $\Sigma$의 주대각원소를 행렬 A의 특이값(singular value)이라고 한다.
- 그리고 이는 대각 방향으로 내림차순으로 정렬되어 있다는 특징을 가진다.

## 2. 절단된 SVD(Truncated SVD)

- LSA에서는 일부 벡터들을 삭제시킨 절단된 SVD를 사용하게 된다.
- 절단된 SVD를 수행하면 값의 손일이 일어나므로 기존의 행렬 A를 복구할 수 없다.
- 하이퍼파라미터란 사용자가 직접 값을 선택하며 성능에 영향을 주는 매개변수를 말한다. 하이퍼파라미터를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 가져갈 수 있지만, 이를 작게 잡아야만 노이즈를 제거할 수 있다.
- 이렇게 일부 벡터들을 삭제하는 것을 데이터의 차원을 줄인다고도 말하는데, 데이터의 차원을 줄이게 되면 당연히 풀 SVD를 하였을 때보다 직관적으로 계산 비용이 낮아지는 효과를 얻을 수 있다.
- 계산 비용이 낮아지는 것 외에도 상대적으로 중요하지 않은 정보를 삭제하는 효과를 갖고 있는데, 이는 영상 처리 분야에서는 노이즈를 제거한다는 의미를 갖고, 자연어 처리 분야에서는 설명력이 낮은 정보를 삭제하고 설명력이 높은 정보를 남긴다는 의미를 갖고 있다.
- 즉, 기존의 행렬에서는 드러나지 않았던 심층적인 의미를 확인할 수 있게 해준다.

## 3. 잠재 의미 분석 (Latent Semantic Analysis, LSA)

- 기존의 DTM이나 DTM에 단어의 중요도에 따른 가중치를 주었던 TF-IDF 행렬은 단어의 의미를 전혀 고려하지 못한다는 단점을 갖고 있었다.
- LSA는 기본적으로 DTM이나, TF-IDF 행렬에 절단된 SVD(truncated SVD)를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 갖고 있다.

```python
import numpy as np
A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])
np.shape(A)
```

```python
U, s, VT = np.linalg.svd(A, full_matrices = True)
```

- numpy의 svd 를 이용하면 쉽게 직교행렬(U), 대각행렬(s), V의 전치행렬 VT를 뽑을 수 있다.

```python
print(U.round(2))
np.shape(U)
'''
[[-0.24  0.75  0.   -0.62]
 [-0.51  0.44 -0.    0.74]
 [-0.83 -0.49 -0.   -0.27]
 [-0.   -0.    1.    0.  ]]
'''
```

```python
# 대각 행렬
print(s.round(2))
np.shape(s)
'''
[2.69 2.05 1.73 0.77]
'''
```

- Numpy의 linalg.svd는 특이값 분해의 결과 대각 행렬이 아니라, 특이값의 리스트를 반환한다.
- 그래서 앞서 본 수식의 형식으로 보려면 이를 다시 대각 행렬로 바꾸어 주어야 한다.

```python
# 대각 행렬 변환
S = np.zeros((4, 9)) # 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성
S[:4, :4] = np.diag(s) # 특이값을 대각행렬에 삽입
print(S.round(2))
np.shape(S)
'''
[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]
'''
```

- 대각 행렬이 생겼고, 내림차순으로 보이는 것을 확인할 수 있다.

```python
print(VT.round(2))
np.shape(VT)
'''
[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]
 [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]
 [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]
 [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]
 [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]
 [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]
 [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]
 [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]
 [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]
'''
```

- 9 _ 9의 크기를 가지는 직교 행렬 VT(V의 전치 행렬)가 생성되었다. 즉, U _ S \* VT를 하면 기존의 행렬 A가 나와야 한다.

```python
np.allclose(A, np.dot(np.dot(U,S), VT).round(2))
# True
```

> 절단된 SVD (Truncated SVD)

```python
# Truncated SVD
S=S[:2,:2]
print(S.round(2))

U=U[:,:2]
print(U.round(2))

VT=VT[:2,:]
print(VT.round(2))

# 이제 기존의 A 행렬로는 복구할 수 없다.
A_prime=np.dot(np.dot(U,S), VT)
print(A)
print(A_prime.round(2))

'''
[[0 0 0 1 0 1 1 0 0]
 [0 0 0 1 1 0 1 0 0]
 [0 1 1 0 2 0 0 0 0]
 [1 0 0 0 0 0 0 1 1]]
[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]
 [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]
 [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]

'''
```

- U, S, VT가 가지는 의미

  축소된 U는 4 _ 2의 크기를 가지는데, 이는 잘 생각해보면 문서의 개수 _ 토픽의 수 t의 크기이다.

  즉, U의 각 행은 잠재 의미를 표현하기 위한 수치화 된 각각의 문서 벡터라고 볼 수 있다.

  축소된 VT는 2 _ 9의 크기를 가지는데, 이는 잘 생각해보면 토픽의 수 t _ 단어의 개수의 크기이다. VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 단어 벡터라고 볼 수 있다.

## 4. LSA의 장단점

- LSA는 쉽고 빠르게 구현이 가능할 뿐만 아니라, 단어의 잠재적인 의미를 이끌어낼 수 있어서 문서의 유사도 계산 등에서 좋은 성능을 보여준다.

# 2. 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)

- 토픽 모델링은 문서의 집합에서 토픽을 찾아내는 프로세스를 말한다.
- 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)은 토픽 모델링의 대표적인 알고리즘이다. 줄여서 LDA라고 한다.
- LDA의 문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다고 가정한다.

## 1. 잠재 디리클레 할당 (Latent Dirichlet Allocation, LDA) 개요

- 문서
  - 1: 저는 사과랑 바나나를 먹어요
  - 2: 우리는 귀여운 강아지가 좋아요
  - 3: 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요
- LDA를 수행할 때, 문서 집합에서 토픽이 몇 개가 존재할지 가정하는 것은 사용자가 해야할 일이다. LDA에 2개의 토픽을 찾으라고 요청하겠다.

  → 이는 k를 2로 한다는 의미이다.

  → 이렇게 모델의 성능에 영향을 주는 사용자가 직접 선택하는 매개변수를 머신 러닝 용어로 하이퍼파라미터라고 한다.

- LDA는 각 문서의 토픽 분포와 각 토픽 내의 단어 분포를 추정한다.

  → 1: 토픽A 100%, 사과 20%, 바나나 40%, 먹어요 40%

  → 2: 토픽B 100%, 귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%

  → 3: 토픽B 60%, 토픽 A 40%,

- LDA는 토픽의 제목을 정해주지는 않지만, 이 시점에서 알고리즘의 사용자는 위 결과로부터 두 토픽이 각각 과일에 대한 토픽과 강아지에 대한 토픽이라고 판단해볼 수 있다.

## 2. LDA의 가정

- LDA는 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘이다.
- LDA는 앞서 배운 빈도수 기반의 표현 방법인 BoW의 행렬 DTM 또는 TF-IDF 행렬을 입력으로 하는데, 이로부터 알 수 있는 사실은 LDA는 단어의 순서는 신경쓰지 않겠다는 것이다.
- LDA는 문서들로부터 토픽을 뽑아내기 위해서 이러한 가정을 염두해두고 있다.
  1. 문서에 사용할 단어의 개수 N을 정한다.
  2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정한다
  3. 문서에 사용할 각 단어를 정한다
     1. 토픽 분포에서 토픽 T를 확률적으로 고른다.
     2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고른다.
- 이러한 과정을 통해 문서가 작성되었다는 가정 하에 LDA는 토픽을 뽑아내기 위하여 위 과정을 역으로 추적하는 역공학(reverse engineering)을 수행한다.

## 3. LDA의 수행하기

- 사용자는 알고리즘에게 토픽의 개수 k를 알려준다.
- 모든 단어를 k개 중 하나의 토픽에 할당한다.
- 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행한다.

  - 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정해보자. 이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당된다.

    p(topic t | document d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율

    p(word w | topic t) : 각 토픽들 t에서 해당 단어 w의 분포

## 4. 잠재 디리클레 할당과 잠재 의미 분석의 차이

- LSA : DTM을 차원 축소 하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.
- LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다.
