# 7. 머신 러닝 (Machine Learning)

- 지금에 이르러 사람들이 말하는 AI는 바로 머신 러닝(Machine Leaning)과 머신 러닝의 한 갈래인 딥 러닝(Deep Leaning)을 의미한다.
- 머신 러닝은 기존에 해결할 수 없었던 수많은 문제들에 대한 최적의 해결방법을 제시해주고 있다.
- 머신 러닝은 이미지 인식, 영상 처리, 알파고와 같은 분야 뿐만 아니라, 자연어 처리에 있어서도 유용하게 쓰인다.

# 1. 머신 러닝이란(What is Machine Learning?)

## 1. 머신 러닝(Machine Learning)이 아닌 접근 방법의 한계

## 2. 머신 러닝은 기존 프로그래밍의 한계에 대한 해결책이 될 수 있다.

- 머신 러닝은 해결을 위한 접근 방법이 기존의 프로그래밍과는 다르다.
- 머신 러닝은 주어진 데이터로부터 결과를 찾는 것에 초점을 맞추는 것이 아니라, 주어진 데이터로부터 규칙성을 찾는 것에 초점이 맞추어져 있다. 주어진 데이터로부터 규칙성을 찾는 과정을 우리는 학습(training)이라고 한다.
- 일단 규칙성을 발견해내면, 그 후에 들어오는 새로운 데이터에 대해서 발견한 규칙성을 기준으로 정답을 찾아내는데, 이는 기존의 프로그래밍 방식으로 접근하기 어려웠던 문제의 해결책이 되기도 한다.

# 2. 머신 러닝 훑어보기

## 1. 머신 러닝 모델의 평가

- 실제 모델을 평가하기 위해서 데이터를 훈련용, 검증용, 테스트용 이렇게 3 가지로 분리하는 것이 일반적이다.
- 검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도이다. 더 정확히는 과적합이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도이다. 하이퍼파라미터(초매개변수)란 값에 따라서 모델의 성능에 영향을 주는 매개변수들을 말한다.
- 반면 가중치와 편향과 같은 학습을 통해 바뀌어져가는 변수를 매개변수라고 부른다.
- 훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며, 하이퍼파라미터를 튜닝(tuning)한다. 또한 이 모델의 매개변수는 검증용 데이터로 정확도가 검증되는 과정에서 점차 검증용 데이터에 점점 맞추어져 가기 시작한다.

## 2. 분류 (Classification)와 회귀(Regression)

- 머신 러닝의 많은 문제는 분류 또는 회귀 문제에 속한다.

### 1. 이진 분류 문제 (Binary Classification)

- 이진 분류는 주어진 입력에 대해서 둘 중 하나의 답을 정하는 문제이다.

  → 시험의 합격, 불합격 / 메일의 정상 메일, 스팸 메일

### 2. 다중 클래스 분류 (Multi-class Classification)

- 다중 클래스 분류는 주어진 입력에 대해서 두 개 이상의 정해진 선택지 중에서 답을 정하는 문제이다.
- 선택지를 주로 카테고리 또는 범주 또는 클래스라고 하며, 주어진 입력으로부터 정해진 클래스 중 하나로 판단하는 것을 다중 클래스 분류 문제라고 한다.

### 3. 회귀 문제 (Regression)

- 분류 문제와 같이 0 또는 1이나 과학 책장, IT 책장 등과 같이 분리된 답이 결과가 아니라, 연속된 값을 결과로 가진다.
- 시계열 데이터를 이용한 주가 예측, 생산량 예측, 지수 예측 등이 이에 속한다.

## 3. 지도 학습 (Supervised Learning)과 비지도 학습 (Unsupervised Learning)

### 1. 지도 학습

- 지도 학습이란 레이블(Label)이라는 정답과 함께 학습하는 것을 말한다.
- 기계는 예측값과 실제값의 차이인 오차를 줄이는 방식으로 학습하게 되는데 예측값은 $\widehat{y}$과 같이 표현하기도 한다.

### 2. 비지도 학습

- 비지도 학습은 레이블이 없이 학습하는 것을 말한다.
- LDA는 비지도 학습에 속한다.

## 4. 샘플(Sample)과 특성(Feature)

- 많은 머신 러닝 문제가 1개 이상의 독립 변수 x를 가지고 종속 변수 y를 예측하는 문제이다.
- 독립 변수 x의 행렬을 X라고 하였을 때, 독립 변수의 개수가 n개 이고 데이터의 개수가 m인 행렬
- 이때 머신 러닝에서는 한아ㅢ 데이터, 하나의 행을 샘플(Sample)이라고 부른다.
- 종속 변수 y를 예측하기 위한 각각의 독립 변수 x를 특성(Feature)이라고 부른다.

## 5. 혼동 행렬 (Confusion Matrix)

- 머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 한다.
- 하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않는다. 이를 위해서 사용하는 것이 혼동 행렬(Confusion Matrix)이다.
- 참 거짓

  참 TP FN

  거짓 FP TN

### 1. 정밀도 (Precision)

- 정밀도는 양성이라고 대답한 전체 케이스에 대한 TP의 비율이다.

  $$정밀도 = \frac{TP}{TP+FP}$$

### 2. 재현율 (Recall)

- 재현율은 실제값이 양성인 데이터의 전체 개수에 대해서 TP의 비율이다. 즉, 양성인 데이터 중에서 얼마나 양성인지를 예측(재현)했는지를 나타낸다.

$$재현율 = \frac{TP}{TP+FN}$$

## 6. 과적합(Overfitting)과 과소 적합(Underfitting)

- 머신 러닝에서 과적합(Overfitting)이란 훈련 데이터를 과하게 학습한 경우를 말한다. 훈련 데이터는 실제로 존재하는 많은 데이터의 일부에 불과하다.
- 그런데 기계가 훈련 데이터에 대해서만 과하게 학습하면 테스트 데이터나 실제 서비스에서의 데이터에 대해서는 정확도가 좋지 않은 현상이 발생한다.
- 과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 높아지는 상황이 발생한다.
- 과적합 방지를 위해 테스트 데이터의 성능이 낮아지기 전에 훈련을 멈추는 것이 바람직하다고 했는데, 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태를 바대로 과소 적합이라고 한다.
- 이러한 두 가지 현상을 과적합과 과소 적합이라고 부르는 이유는 머신 러닝에서 학습 또는 훈련이라고 하는 과정을 적합(fitting)이라고도 부를 수 있기 때문이다. 모델이 주어진 데이터에 대해서 적합해져가는 과정이기 때문이다.

# 3. 선형 회귀 (Linear Regression)

## 1. 선형 회귀 (Linear Regression)

- 변수 x의 값은 독립적으로 변할 수 있는 것에 반해, y값은 계속해서 x의 값에 의해서, 종속적으로 결정되므로, x를 독립 변수, y를 종속 변수라고도 한다.
- 선형 회귀는 한 개 이상의 독립 변수 x와 y의 선형 관계를 모델링한다.

### 1. 단순 선형 회귀 분석 (Simple Linear Regression Analysis)

$$y = W_x + b$$

- 독립 변수 x와 곱해지는 값 W를 머신 러닝에서는 가중치,
- 별도로 더해지는 값 b를 편향(bias)라고 한다.
- W(Weight)와 b(bias)가 없이 y와 x란 수식은 y는 x와 같다는 하나의 식 밖에 표현하지 못한다.
- 다시 말해 W와 b의 값을 적절히 찾아내면 x와 y의 관계를 적절히 모델링한 것이 된다.

### 2. 다중 선형 회귀 분석 (Multiple Linear Regression Analysis)

- y는 하나이지만, x가 여러개 일 때, 이를 선형 회귀 분석이라고 한다. 편향은 여전히 하나

## 2. 가설 (Hypothesis) 세우기

- 알고있는 데이터로부터 x와 y의 관계를 유추하고, 후에 상황을 예측해보고 싶을 때 수학적으로 식을 세워보게 되는데 머신 러닝에서는 이러한 식을 가설 이라고 한다.

$$H(x)=W_x+b$$

- 이 식은 W와 b의 값에 따라서 천차만별로 그려지는 직선의 모습이 보여진다.
- 선형회귀는 주어진 데이터로부터 y와 x의 관계를 가장 잘 나타내는 직선을 그리는 일을 한다.
- 그리고 어떤 직선인지 결정하는 것은 W와 b의 값이므로, 선형 회귀에서 해야할 일은 결국 적절한 W와 b를 찾아내는 일이된다.
- 적절한 W와 b를 찾게되면 후에 상황을 예측하는 것이 가능해진다.

## 3. 비용 함수 (cost function) : 평균 제곱 오차(MSE)

- 가설을 세웠다면 W와 b를 이용하여 식을 세우는 일이 필요하다.
- 머신 러닝은 W와 b를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 W와 b를 찾아낸다.
- 이 때 실제값과 예측값에 대한 오차에 대한 식을 목적 함수(Objective function) 또는 비용 함수 (Cost function) 또는 손실 함수(Loss Function)라고 한다.
- 목적함수: 함수의 값을 최소화 하거나 최대화하거나 하는 목적을 가진 함수
- 비용함수, 손실함수: 값을 최소화하기 위한 함수
- 비용 함수는 단순히 실제값과 예측값의 대한 오차를 표현하면 되는 것이 아니라, 예측값의 오차를 줄이는 일에 최적화 된 식이어야 한다.
- 각 문제들에는 적합한 비용 함수들이 있다.
- 회귀 문제의 경우에는 주로 평균 제곱 오차(Mean Squared Error, MSE)가 사용된다.

![https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC3.PNG](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC3.PNG)

- 해당 선은 W의 값 13과 임의의 b의 값 1을 가진 직선을 그린것이다.
- 이제 이 직선으로부터 서서히 W와 b의 값을 바꾸면서 정답인 직선을 찾아내야 한다.
- y와 x의 관계를 가장 잘 나태는 직선을 그린다는 것은 위의 그림에서 모든 점들과 위치적으로 가장 가까운 직선을 그린다는 것과 같다.
- 여기서 오차(err)를 정의한다.

  (25,27,-2), (50,40,10), (42,53,-9), (61,66,-5)

  여기서 오차는 모든 오차를 제곱하여 더하는 방법을 사용한다.

  $$\Sigma^n_{i=1}[{y^{(i)} -H_{(x^{(i)})}}]^2 = (-2)^2 + 10^2 + (-9)^2+(-5)^2 = 210$$

  이때 데이터 개수인 n으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있는데, 이를 평균 제곱 오차(MSE)라고 한다.

  $$210/4 = 52.5$$

- **이렇듯 평균 제곱 오차의 값을 최소값으로 만드는 W와 b를 찾아내는 것이 정답인 직선을 찾는 일이다.**
- 평균제곱오차 식

  $$cost(W, b) = \frac{1}{n}\Sigma^n_{i=1}[{y^{(i)} -H_{(x^{(i)})}}]^2 $$

## 4. 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)

- 선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 W와 b를 찾기 위한 작업을 수행한다. 이때 사용되는 알고리즘을 옵티마이저(Optimizer) 또는 최적화 알고리즘이라고 부른다.
- 이 옵티마이저를 통해 적절한 W와 b를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 부른다.

> 경사 하강법의 이해

- cost와 기울기 W와의 관계

  - W는 머신러닝용어로는 가중치이지만, 직선의 방적식 관점에서 보면 직선의 기울기를 의미하고 있다.

  ![https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC4.PNG](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC4.PNG)

  - 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제 값과 예측값의 오차가 커진다.

  ![https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%99%80%EC%BD%94%EC%8A%A4%ED%8A%B8.PNG](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%99%80%EC%BD%94%EC%8A%A4%ED%8A%B8.PNG)

  - 이에 대응하여 W가 무한히 커지거나 작아지면 cost의 값도 무한대로 커진다.
  - 기계는 임의의 랜덤값 W값을 정한뒤에, 맨 아래의 볼록한 부분을 향해 점차 W의 값을 수정해 나간다. 이를 가능케 하는 것이 경사 하강법(Gradient Descent)이다.

  ![https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG)

  - 초록색 선은 W가 임의의 값을 가지게 되는 4가지 경우에 대해서, 그래프 상으로 접선의 기울기를 보여준다. 접선의 기울기가 점차 작아지는 것을 볼 수 있다.
  - 맨 아래의 볼록한 부분에서는 결국 접선의 기울기가 0이된다.
  - 즉, cost가 최소화 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이되는 지점이다.

  $$cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2$$

  - 비용 (cost)를 최소화 하는 W를 구하기 위해 W를 업데이트하는 식은 다음과같다. 접선의 기울기가 0이 될때까지 반복한다.

    $$W := W - α\frac{∂}{∂W}cost(W)$$

    $α$는 여기서 학습률이라고 하는데, 우선 이는 생각하지 않고, 현재 W에서 현재 W에서의 접선의 기울기를 빼는 행위가 어떤 의미가 있는지 보자

    $$W := W - α(음수 기울기) = W + α(양수 기울기)$$

    기울기가 음수면 W의 값이 증가하게 되는데, 이는 결과적으로 접선의 기울기가 0인 방향으로 W의 값이 조정된다.

    기울기가 양수라면 위의 수식은 아래와 같이 표현될 수 있다.

    $$W := W - α(양수 기울기)$$

    기울기가 양수면 W의 값이 감소하게 되는데 이는 결과적으로 기울기가 0인 방향으로 W의 값이 조정된다.

    결국 위에서 봤던 수식은 접선의 기울기가 음수거나 양수일 때 모두 접선의 기울기가 0의 방향으로 W의 값을 조정한다.

  - 여기서 학습률 (learning rate)라고 말하는 $α$는 W의 값을 변경할 때, 얼마나 크게 변경할지를 결정한다. 또는 W를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정한다.

# 4. 로지스틱 회귀 (Logistic Regression)

- 둘 중 하나를 결정하는 문제를 이진 분류(Binary Classfication)라고 한다. 그리고 이런 문제를 풀기 위한 대표적인 알고리즘으로 로지스틱 회귀(Logistic Regression)이 있다.

## 1. 이진 분류 (Binary Classfication)

- 앞서 선형 회귀 챕터에서 공부 시간과 성적 간의 관계를 직선의 방정식으로 표현한다는 가설 하에, 주어진 데이터로부터 가중치 $W$(weight)와 편향 $b$(bias)를 찾아 데이터를 가장 잘 표현하는 직선을 찾았다.
- 학생들이 시험 성적에 따라서 합격 불합격이 기재된 데이터가 있을 때, 이는 직선으로 표현하는 것이 적절하지 않다.
- 해당 데이터는 아래와 같이 S자 형태로 표현이 되기 때문이다.

![https://wikidocs.net/images/page/22881/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.PNG](https://wikidocs.net/images/page/22881/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.PNG)

- 이러한 x와 y의 관계를 표현하기 위해서는 직선을 표현하는 함수가 아니라, S자 형태로 표현할 수 있는 함수가 필요하다.
- 또한 실제값 y가 0또는 1이라는 2가지 값 밖에 가지지 않으므로, 이 문제를 풀기 위해서는 예측값이 0과 1사이의 값을 가지도록 하는 것이 보편적이다.
- 최종 예측값이 0.5 보다 작으면 0으로 예측했다고 판단하고, 0.5보다 크면 1로 예측했다고 판단할 수 있는데, 선형 회귀의 경우 y값이 음의 무한대부터 양의 무한대와 같은 큰 수들도 가질 수 있는데, 이는 분류 문제에 적합하지 않다.
- 0과 1사이의 값을 가지면서, S자 형태로 그려지는 조건을 충족하는 함수가 있다. 바로 시그모이드 함수 이다.

## 2. 시그모이드 함수 (Sigmoid function)

- 시그모이드 함수는 종종 σ로 축약해서 표현하기도 한다.
- 위의 문제를 풀기 위한 가설식이기도 하고, sigmoid 함수의 방정식이기도 하다.

$$H(X) = \frac{1}{1 + e^{-(Wx + b)}} = sigmoid(Wx + b) = σ(Wx + b)$$

- 여기서 e(e=2.718281...)는 자연 상수이다. 여기서 구해야할 것은 여전히 주어진 데이터에 가장 적합한 가중치 W(weight)와 편향 b(bias)이다.
- **인공지능 알고리즘이 하는 것은 결국 주어진 데이터에 적합한 가중치 W와 b를 구하는 것**이다.
- 이는 W는 1, b는 0임을 가정한 그래프이다.

![https://wikidocs.net/images/page/22881/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%EA%B7%B8%EB%9E%98%ED%94%84.png](https://wikidocs.net/images/page/22881/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%EA%B7%B8%EB%9E%98%ED%94%84.png)

- 이 그래프에서의 시그모이드 함수는 출력값을 0과 1사이의 값으로 조정하여 반환한다. 마치 S자의 모양을 연상시킨다.
- x가 0일 때, 0.5의 값을 가진다. x가 증가하면 1에 수렴한다.
- 이때 구해야 할 가중치와 편향이 어떤 의미를 가지는지 그래프를 통해 알아보자.

![https://wikidocs.net/images/page/22881/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%98%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%9D%98%EB%B3%80%ED%99%94.png](https://wikidocs.net/images/page/22881/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%98%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%9D%98%EB%B3%80%ED%99%94.png)

- W의 값이 0.5일 때 빨간색선, W의 값이 1일때는 초록색선, W의 값이 2일때는 파란색선이 나오도록 했다. 이는 W의 값에 따라 그래프의 경사도가 변하는 것을 볼 수 있었다. (앞서 선형 회귀에서 가중치 W는 직선의 기울기를 의미했지만, 여기서는 그래프의 경사도를 결정한다.)
- 이제 b를 변경해보자.

![https://wikidocs.net/images/page/22881/b%EC%9D%98%EC%9D%B4%EB%8F%99.png](https://wikidocs.net/images/page/22881/b%EC%9D%98%EC%9D%B4%EB%8F%99.png)

- 이는 b의 값에 따라서 그래프가 이동하는 것을 보여준다.
- 시그모이드 함수는 입력값이 커지면 1에 수렴하고, 입력값이 작아지면 0에 수렴한다. 0부터 1까지의 값을 가지는데, 출력값이 0.5 이상이면 1(True), 0.5 이하면 0(False)로 만들면 이진 분류 문제로 사용할 수 있다.
- 이를 확률의 개념으로 바라보면 해당 범주에 속할 확률이 50%가 넘으면, 해당 범주라고 판단하고, 50%보다 낮으면 아니라고 판단한다고도 볼 수 있다.

## 3. 비용 함수 (Cost Function)

- 로지스틱 회귀 또한 경사 하강법을 사용하여 가중치 W를 찾아내지만, 비용 함수로는 평균제곱 오차를 사용하지 않는다.
- 로지스틱 회귀에서 평균 제곱 오차를 비용 함수로 사용하면, 경사 하강법을 사용하였을 때 자칮 잘못하면 찾고자 하는 최소값이 아닌, 잘못된 최소값에 빠진다. 이를 전체 함수에 걸쳐 최소값인 글로벌 미니멈(Global Minimum)이 아닌, 특정 구역에서의 최소값인 로컬 미니멈(Local Minimum)에 도달했다고 한다.

  → 이는 cost가 최소가 되는 가중치 W를 찾는다는 비용 함수의 목적에 맞지 않는다.

$$J(W) = \frac{1}{n} \sum_{i=1}^{n} cost\left(H(x^{(i)}), y^{(i)})\right)$$

- 시그모이드 함수는 0과 1사이의 y값을 반환한다. 이는 실제 값이 0일 때, y값이 1에 가까워지면 오차가 커지며, 실제값이 1일 때 y값이 0에 가까워지면 오차가 커짐을 의미한다.
- 실제값이 1일 때, 예측값의 값이 1이면 오차가 0이므로 당연히 cost는 0이된다. 반면 실제값이 1일 때, 예측값이 0으로 수렴하면 cost는 무한대로 발산한다. 실제값이 0인 경우는 그 반대로 이해하면 된다.
- 결과적으로 로지스틱 회귀의 목적 함수는 아래와 같다.

$$J(W) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]$$

- 이때 로지스틱 회귀에서 찾아낸 비용 함수를 크로스 엔트로피(Cross Entropy)함수라고 한다. 즉, 결론적으로 로지스틱 회귀는 비용 함수로 크로스 엔트로피 함수를 사용하며, 가중치를 찾기 위해서 크로스 엔트로피 함수의 평균을 취한 함수를 사용한다.

# 5. 다중 입력에 대한 실습

## 1. 다중 선형 회귀

- Midterm($x_1$), Final($x_2$), Added point($x_3$), Score($1000)($y$)
- 위와 같이 y를 결정하는데 있어 독립 변수가 3개인 선형회귀 (3차원 벡터)의 구조일 때, 중간 고사, 기말 고사, 그리고 추가 점수를 어떤 공식을 통해 최종 점수가 계산되는지 실습해보자.

$$H(X) = {W_1x_1 + W_2x_2 + W_3x_3 + b}$$

- 상위 5개의 데이터만 훈련에 사용하고, 나머지 2개는 테스트에 사용해보도록 하자. (분할 하여 사용)

> 데이터 설정

```python
# 입력 벡터의 차원은 3입니다. 즉, input_dim은 3입니다.
X = np.array([[70,85,11],[71,89,18],[50,80,20],[99,20,10],[50,10,10]]) # 중간, 기말, 가산점

# 출력 벡터의 차원은 1입니다. 즉, output_dim은 1입니다.
y = np.array([73,82,72,57,34]) # 최종 성적
```

> 다음과 같이 y를 결정 짓는데, 3차원의 벡터를 사용할 경우, keras model의 input_dim 차원을 3차원으로 설정해주면 된다.

```python
model=Sequential()
model.add(Dense(1, input_dim=3, activation='linear'))
```

> 학습률, 손실함수, 최적점 찾기 시도 (학습)

```python
# 학습률(learning rate, lr)은 0.00001로 합니다.
sgd=optimizers.SGD(lr=0.00001)

# 손실 함수(Loss function)은 평균제곱오차 mse를 사용합니다.
model.compile(optimizer = sgd ,loss='mse',metrics=['mse'])

# 주어진 X와 y데이터에 대해서 오차를 최소화하는 작업을 2,000번 시도합니다.
model.fit(X,y, batch_size=1, epochs=2000, shuffle=False)
```

> 학습용 데이터로 테스팅

```python
print(model.predict(X))
```

> 테스팅용 데이터로 테스팅

```python
X_test = np.array([[20,99,10],[40,50,20]]) # 각각 58점과 56점을 예측해야 합니다.
print(model.predict(X_test))
```

## 2. 다중 로지스틱 회귀

- y를 결정하는데 있어 독립 변수 x가 2개인 로지스틱 회귀를 실습해보자
- SepallLengthCm($x_1$), PetalLengthCm($x_2$), Species($y$)
- Referenc의 예제를 보면 알겠지만 y의 값은 A,B 0과 1로 나누어진다.
- 그러면 가설의 공식은 다음과 같을 것이다.

$$H(X) = Sigmoid(W_1x_1 + W_2x_2 + b)$$

- 이는 OR gate로 구현을 하면 된다.

> 데이터 정의

```python
# 입력 벡터의 차원은 2입니다. 즉, input_dim은 2입니다.
X=np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

# 출력 벡터의 차원은 1입니다. 즉, output_dim은 1입니다.
y=np.array([0, 1, 1, 1])
```

> 모델 정의 (입력 차원 정의, sigmoid 함수를 활성화 함수로 이용)

```python
model=Sequential()
model.add(Dense(1, input_dim=2, activation='sigmoid')) # 이제 입력의 차원은 2입니다.
```

> binary_crossentropy 이용

```python
# sgd는 경사 하강법을 의미.
# 손실 함수(Loss function)는 binary_crossentropy(이진 크로스 엔트로피)를 사용합니다.
model.compile(optimizer='sgd' ,loss='binary_crossentropy',metrics=['binary_accuracy'])

# 주어진 X와 y데이터에 대해서 오차를 최소화하는 작업을 800번 시도합니다.
model.fit(X, y, batch_size=1, epochs=800, shuffle=False)
```

> 테스팅 확인

```python
print(model.predict(X))
```

- 입력으로 0,0 인 경우를 제외하고, 나머지 3개의 입력 쌍에 대해서는 전부 값이 0.5를 넘는 것을 확인할 수 있다.

## 3. 인공 신경망 다이어그램

- 로지스틱 회귀는 일종의 인공 신경망 구조로 해석해도 무방하다.

$$y = sigmoid(W_1x_1 + W_2x_2 + W_3x_3 + ... + W_nx_n + b) = σ(W_1x_1 + W_2x_2 + W_3x_3 + ... + W_nx_n + b)$$

# 6. 벡터와 행렬 연산

- 앞서 독립 변수 x가 2개 이상인 선형 회귀와 로지스틱 회귀에 대해서 배웠다.
- 하지만 소프트맥스 회귀에서는 종속 변수 y의 종류가 3개 이상이 되면서 더욱 복잡해진다. 또한 이러한 식들이 겹겹이 누적되면 인공 신경망의 개념이 된다.

## 1. 벡터와 행렬과 텐서

- **벡터는 크기와 방향을 가진 양이다. 숫자가 나열된 형상**이며, 파이썬에서는 1차원 배열 또는 리스트로 표현한다.
- 반면 행렬은 행과 열을 가지는 2차원 형상을 가진 구조이다. 파이썬에서는 2차원 배열로 표현한다.
- 3차원부터는 주로 텐서라고 부른다. 텐서는 파이썬에서는 3차원 이상의 배열로 표현한다.

## 2. 텐서 (Tensor)

- 인공 신경망은 복잡한 모델 내의 연산을 주로 행렬 연산을 통해 해결한다.
- 여기서 행렬 연산이란 머신 러닝의 입, 출력이 복잡해지면 3차원 텐서에 대한 이해가 필수로 요구된다.
- **인공신경망 모델 중 하나인 RNN에서는 3차원 텐서에 대한 개념을 모르면 RNN을 이해하기가 쉽지 않다.**

### 1. 0차원 텐서

- 스칼라는 하나의 실수값으로 이루어진 데이터를 말한다. 또한 스칼라값을 0차원 텐서라고 한다.
- 0D(Dimesionality) 텐서 라고도 한다.

```python
# 0차원 텐서
d=np.array(5)
print(d.ndim) # 차원수 출력
# 0
print(d.shape) # 텐서의 크기 출력
# ()
```

### 2. 1차원 텐서

- 숫자를 특정 순서대로 배열한 것을 벡터라고 한다.
- 주의할 점은 벡터의 차원과 텐서의 차원은 다른 개념이다.
- 아래의 예제는 4차원 벡터이지만, 1차원 텐서이다.

```python
# 1차원 텐서
d=np.array([1, 2, 3, 4])
print(d.ndim) # 1
print(d.shape) # (4, )
```

- 벡터에서의 차원은 하나의 축에 차원들이 존재하는 것이고, 텐서에서의 차원은 축의 개수를 의미한다.

### 3. 2차원 텐서

- 행과 열이 존재하는 벡터의 배열. 즉, 행렬(matrix)을 2차원 텐서라고 한다.

```python
# 2차원 텐서
d=np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
print(d.ndim) # 2
print(d.shape) # (3, 4)
```

- 1차원 텐서를 벡터, 2차원 텐서를 행렬로 비유하였는데, 수학적으로 행렬의 열을 열벡터로 부르거나, 열벡터를 열행렬로 부르는 것과 혼동해서는 안된다. 여기서 말하는 1차원 텐서와 2차원 텐서는 차원 자체가 달라야 한다.

### 4. 3차원 텐서

- 행렬 또는 2차원 텐서를 단위로 한 번 더 배열하면 3차원 텐서라고 부른다.
- 3차원 이상의 텐서부터 본격적으로 텐서라고 부른다.

```python
# 3차원 텐서
d=np.array([
            [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [10, 11, 12, 13, 14]],
            [[15, 16, 17, 18, 19], [19, 20, 21, 22, 23], [23, 24, 25, 26, 27]]
            ])
print(d.ndim) # 3
print(d.shape) # (2, 3, 5)
```

- 3차원 텐서의 구조는 복잡한 인공 신경망의 입, 출력값을 이해하는 근본이 된다.
- 자연어 처리에서 특히 자주 보게 되는 것이 이 3D텐서이다. **3D 텐서는 시퀀스 데이터를 표현할 때 자주 사용되기 때문이다.**
- 시퀀스는 주로 문장이나 문서, 뉴스 기사 등의 텍스트가 될 수 있다. 이 경우 3D 텐서는 samples, timesteps, word_dim 이 된다.
- 또는 일괄로 처리하기 위해 데이터를 묶는 단위인 배치의 개념에 대해서보면 batch_size, timesteps, word_dim 이라고도 볼 수 있다.
- samples/batch_size는 데이터의 개수, timesteps는 시퀀스의 길이, word_dim은 단어를 표현하는 벡터의 차원을 의미한다.
- 자연어 처리에서는 훈련 데이터를 모두 원-핫 벡터로 벡터화를 진행해야 하는데, 이렇게 벡터화한 데이터를 인공 신경망의 입력으로 한 꺼번에 사용하는 것을 배치라고 한다.

### 5. 그 이상의 텐서

- 3차원 텐서를 배열로 합치면 4차원 텐서가 된다. 4차원 텐서를 배열로 합치면 5차원 텐서가 된다. 이런 식으로 텐서는 배열로서 계속해서 확장할 수 있다.

### 6. 케라스에서의 텐서

- 케라스에서는 입력의 크기(shape)를 인자로 줄 때, input_shape라는 인자를 사용한다.
- input_shape는 배치 크기를 제외하고 차원을 지정하는데, 예를 들어 input_shape(6,5)라는 인자값을 사용하고 배치 크기를 32라고 지정한다면 이 텐서의 크기는 (32,6,5)를 의미한다.
- 만약에 배치 크기까지 지정해주고 싶다면 batch_input_shape = (8,2,10)와 같이 사용한다.
- 그 외에도 입력의 속성 수를 의미하는 input_dim, 시퀀스 데이터의 길이를 의미하는 input_length 등의 인자도 사용한다. 사실 input_shape의 두 개의 인자는 (input_length, input_dim)라고 볼 수 있다.

## 3. 벡터와 행렬의 연산

### 1. 벡터와 행렬의 덧셈과 뺄셈

$$a + b = \left[    \begin{array}{c}      8 \\      4 \\      5 \\    \end{array}  \right]+ \left[    \begin{array}{c}      1 \\      2 \\      3 \\    \end{array}  \right]= \left[    \begin{array}{c}      9 \\      6 \\      8 \\    \end{array}  \right]$$

$$a - b = \left[    \begin{array}{c}      8 \\      4 \\      5 \\    \end{array}  \right]- \left[    \begin{array}{c}      1 \\      2 \\      3 \\    \end{array}  \right]= \left[    \begin{array}{c}      7 \\      2 \\      2 \\    \end{array}  \right]$$

- 같은 크기의 두 개의 벡터나 행렬은 덧셈과 뺄셈을 할 수 있다. 이 경우 같은 위치의 원소끼리 연산하면 된다. 이러한 연산을 요소별(element-wise) 연산이라고 한다.

```python
a = np.array([8, 4, 5])
b = np.array([1, 2, 3])
print(a+b)
print(a-b)
'''
[9 6 8]
[7 2 2]
'''
```

- 행렬도 마찬가지이다.

$$a + b = \left[    \begin{array}{c}      10\ 20\ 30\ 40\\      50\ 60\ 70\ 80\\    \end{array}  \right] + \left[    \begin{array}{c}      5\ 6\ 7\ 8\\      1\ 2\ 3\ 4\\    \end{array}  \right]= \left[    \begin{array}{c}      15\ 26\ 37\ 48\\      51\ 62\ 73\ 84\\    \end{array}  \right]$$

$$a - b = \left[    \begin{array}{c}      10\ 20\ 30\ 40\\      50\ 60\ 70\ 80\\    \end{array}  \right] - \left[    \begin{array}{c}      5\ 6\ 7\ 8\\      1\ 2\ 3\ 4\\    \end{array}  \right]= \left[    \begin{array}{c}      5\ 14\ 23\ 32\\      49\ 58\ 67\ 76\\    \end{array}  \right]$$

```python
a = np.array([[10, 20, 30, 40], [50, 60, 70, 80]])
b = np.array([[5, 6, 7, 8],[1, 2, 3, 4]])
print(a+b)
print(a-b)
'''
[[15 26 37 48]
 [51 62 73 84]]
[[ 5 14 23 32]
 [49 58 67 76]]
'''
```

### 2. 벡터의 내적과 행렬의 곱셈

- 벡터의 점곱(dot product) 또는 내적(inner product)에 대해 알아보자.
- 벡터의 내적은 연산을 점(dot)으로 표현하여 $a \cdot b$와 같이 표현하기도 한다.
- 내적이 성립하기 위해서는 두 벡터의 차원이 같아야 하며, 두 벡터 중 앞의 벡터가 행벡터이고, 뒤의 벡터가 열벡터여야 한다.
- 벡터의 내적의 결과는 스칼라가 된다는 특징이 있다.

$$a \cdot b =\left[    \begin{array}{c}      1\ 2\ 3    \end{array}  \right]\left[    \begin{array}{c}      4 \\      5 \\      6 \\    \end{array}  \right]= 1 × 4 + 2 × 5 + 3 × 6 = 32\text{(스칼라)}$$

```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
print(np.dot(a,b)) # 32
```

- 행렬의 곱셈을 이해하기 위해서는 벡터의 내적을 이해해야 한다. 행렬의 곱셈은 왼쪽 행렬의 행벡터(가로 방향 벡터)와 오른쪽 행렬의 열벡터(세로 방향 벡터)의 내적(대응하는 원소들의 곱의 합)의 결과로 행렬의 원소가 되는 것으로 이루어진다.

$$ab =\left[    \begin{array}{c}      1\ 3\\      2\ 4\\    \end{array}  \right]\left[    \begin{array}{c}      5\ 7\\      6\ 8\\    \end{array}  \right]= \left[    \begin{array}{c}      1 × 5 + 3 × 6\ \ \ 1 × 7 + 3 × 8\\      2 × 5 + 4 × 6\ \ \ 2 × 7 + 4 × 8\\    \end{array}  \right]=\left[    \begin{array}{c}      23\ 31\\      34\ 46\\    \end{array}  \right]$$

```python
a = np.array([[1, 3],[2, 4]])
b = np.array([[5, 7],[6, 8]])
print(np.matmul(a,b))
```

## 3. 다중 선형 회귀 행렬 연산으로 이해하기

- 독립 변수가 2개 이상일 때, 1개의 종속 변수를 예측하는 문제를 행렬의 연산으로 표현한다면 어떻게 될까? 다중 선형 회귀나 다중 로지스틱 회귀가 이러한 연산의 예이다.

$$y = w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b$$

- 위의 식은 입력 벡터와 가중치 벡터의 내적으로, 혹은 가중치 벡터와 입력 벡터의 내적으로 표현할 수 있다.

$$y = \left[    \begin{array}{c}      x_{1}\ x_{2}\ x_{3}\ \cdot\cdot\cdot\ x_{n}    \end{array}  \right]\left[    \begin{array}{c}      w_{1} \\      w_{2} \\      w_{3} \\      \cdot\cdot\cdot \\      w_{n}    \end{array}  \right]+b= x_1w_1 + x_2w_2 + x_3w_3 + ... + x_nw_n + b$$

$$y = \left[    \begin{array}{c}      w_{1}\ w_{2}\ w_{3}\ \cdot\cdot\cdot\ w_{n}    \end{array}  \right]\left[    \begin{array}{c}      x_{1} \\      x_{2} \\      x_{3} \\      \cdot\cdot\cdot \\      x_{n}    \end{array}  \right]+b= x_1w_1 + x_2w_2 + x_3w_3 + ... + x_nw_n + b$$

- 데이터의 개수가 많은 경우에는 벡터의 내적이 아니라, 행렬의 곱셈으로 표현이 가능하다.

$$\left[    \begin{array}{c}      x_{11}\ x_{12}\ x_{13}\ x_{14} \\      x_{21}\ x_{22}\ x_{23}\ x_{24} \\      x_{31}\ x_{32}\ x_{33}\ x_{34} \\      x_{41}\ x_{42}\ x_{43}\ x_{44} \\      x_{51}\ x_{52}\ x_{53}\ x_{54} \\    \end{array}  \right]\left[    \begin{array}{c}      w_{1} \\      w_{2} \\      w_{3} \\      w_{4} \\    \end{array}  \right]  =\left[    \begin{array}{c}      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}+ x_{14}w_{4} \\      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}+ x_{24}w_{4} \\      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}+ x_{34}w_{4} \\      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}+ x_{44}w_{4} \\      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}+ x_{54}w_{4} \\    \end{array}  \right]$$

- 여기에 편향 벡터 B를 더 해주면 위 데이터에 대한 전체 가설 수식 H(X)를 표현할 수 있다.

$$\left[    \begin{array}{c}      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}+ x_{14}w_{4} \\      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}+ x_{24}w_{4} \\      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}+ x_{34}w_{4} \\      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}+ x_{44}w_{4} \\      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}+ x_{54}w_{4} \\    \end{array}  \right]+\left[    \begin{array}{c}      b \\      b \\      b \\      b \\      b \\    \end{array}  \right]= \left[    \begin{array}{c}      y_{1}\\ y_{2}\\ y_{3}\\ y_{4}\\ y_{5} \\    \end{array}  \right]$$

- 가중치 벡터를 앞에 두고 입력 행렬을 뒤에 두고 행렬 연산을 한다면 이는 아래와 같다.

$$\left[    \begin{array}{c}      w_{1}\ w_{2}\ w_{3}\ w_{4} \\    \end{array}  \right]\left[    \begin{array}{c}      x_{11}\ x_{21}\ x_{31}\ x_{41}\ x_{51}\\      x_{12}\ x_{22}\ x_{32}\ x_{42}\ x_{52}\\      x_{13}\ x_{23}\ x_{33}\ x_{43}\ x_{53}\\      x_{14}\ x_{24}\ x_{34}\ x_{44}\ x_{54}\\    \end{array}  \right]+\left[    \begin{array}{c}      b\ b\ b\ b\ b \\    \end{array}  \right]=\left[    \begin{array}{c}      y_{1}\ y_{2}\ y_{3}\ y_{4}\ y_{5} \\    \end{array}  \right]$$

## 4. 샘플(Sample)과 특성(Feature)

![https://wikidocs.net/images/page/35821/n_x_m.PNG](https://wikidocs.net/images/page/35821/n_x_m.PNG)

- 훈련 데이터의 입력 행렬을 X라고 했을 때, 샘플(Sample)과 특성(Feature)의 정의를 머신러닝에서는 데이터를 셀 수 있는 단위로 구분할 때, 각각을 샘플이라고 부르며, 종속 변수 y를 예측하기 위한 각각의 독립 변수 x를 특성이라고 부른다.

## 5. 가중치와 편향 행렬의 크기 결정

- 특성을 행렬의 열로 보는 경우를 가정하여 행렬의 크기가 어떻게 결정되는지를 정리
- 행렬곱은 2가지 정의를 가지는데, 두 개의 행렬 J와 K의 곱은 다음과 같은 조건이 만족되어야 한다.
  1. 두 행렬의 곱 J x K 에 대하여 행렬 J의 열의 수와 행렬 K의 행의 수는 같아야 한다.
  2. 두 행렬의 곱 J x K 의 결과로 나온 행렬 JK의 크기는 J의 행의 크기와 K의 열의 크기를 가진다.
- 이 두가지 특성을 가지고 주어진 데이터가 입력과 출력의 행렬의 크기를 어떻게 가지느냐에 따라서 가중치 W의 행렬과 편향 b의 행렬의 크기를 찾아낼 수 있다.

$$X_{m * n} * W_{?*?} + B_{?*?}=Y_{m*j}$$

- 다음과 같은 구조에서 W행렬과 B행렬의 크기를 추론해보자면
- 첫번째로, 행렬간의 덧셈은 행렬간의 크기가 같아야 한다. 따라서 B의 크기는 $B_{m*j}$이다
- 두번째로, X와 W를 곱하려면 W는 앞에 있는 행렬의 열의 크기와 행 수가 같아야 한다. 따라서 W의 행의 크기는 n 이다.
- 세번째로, W와 B의 덧셈은 행렬의 크기가 같아야 한다.이 때, 행렬의 곱에서 뒤에 있는 행렬의 열의 크기와 동일해야 한다는 것을 말한다. 따라서 W의 열의 크기는 j로 W의 크기는 $W_{n*j}$이다.
- 이 때 위의 식에서 X행렬의 행을 의미하는 수치 m은 샘플 데이터를 몇 개씩 묶어서 처리하느냐에 따라 달라진다.
- 전체 샘플 데이터 중 1개씩 불러와서 처리하고자한다면 m은 1이된다. 이렇게 기계가 임의의 m개씩 묶인 작은 그룹들로 분할하여 여러번 처리하는 것을 미니배치 학습이라고 한다.
- 이를 또한 배치 크기(Batch size)라고도 한다.

# 7. 소프트맥스 회귀 (Softmax Refression) - 다중 클래스 분류

## 1. 다중 클래스 분류 (Multi-class Classification)

- 이진 분류가 2 개의 선택지 중 하나를 고르는 문제였다면, 세 개 이상의 선택지 중 하나를 고르는 문제를 다중 클래스 분류라고 한다.
- 꽃받침 길이 (setosa), 꽃잎 길이 (versicolor), 꽃잎 넓이 (virginica)라는 3개의 품종 중 어떤 품종인지를 예측하는 문제일 때,
- 예제를 보면 알다 시피 3개 이상의 정답지 중에 고르는 문제이다. 시그모이드를 적용한다 치면, 첫번째가 정답일 확률은 0.7, 두번째가 정답일 확률은 0.6, 세번째가 정답일 확률은 0.4 등과 같은 출력을 얻게 된다. 그런데 이 전체 확률의 합계가 1이 되도록 하여 전체 정답지에 걸친 확률로 바꿀 수는 없을까?
- 만약 하나의 샘플 데이터에 대한 예측값으로 모든 가능한 정답지에 대해서 정답일 확률의 합이 1이 되도록 구할 수 있다면 어떨까? 이럴 때 사용할 수 있는 것이 소프트맥스 함수이다.

  → setosa일 확률 0.58, versicolor일 확률 0.22, virginica일 확률 0.2

## 2. 소프트맥스 함수 (Softmax function)

- 소프트맥스 함수는 분류해야하는 정답지(클래스)의 총 개수를 k라고 할 때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정한다.

### 1. 이해

- k차원의 벡터에서 i번째 원소를 $z_i$, i번째 클래스가 정답일 확률을 $p_i$로 나타낸다고 하였을 때, 다음과 같이 정의가 된다.

$$p_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{k} e^{z_{j}}}\ \ for\ i=1, 2, ... k$$

- 위의 문제에서는 k=3이므로, 3차원 벡터 $z=[z_1, z_2, z_3]$의 입력을 받으면 소프트맥스 함수는 아래와 같은 출력을 리턴한다.

$$softmax(z)=[\frac{e^{z_{1}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{2}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{3}}}{\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = \hat{y} = \text{예측값}$$

- $\hat{y}$는 각각의 클래스가 정답일 확률을 나타낸다.
- 순서는 문제를 풀고자 하는 사람의 무작위 선택이다. 이에 따라 식을 문제에 맞게 다시 써보자

$$softmax(z)=[\frac{e^{z_{1}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{2}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{3}}}{\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = [p_{virginica}, p_{setosa}, p_{versicolor}]$$

- 간단히 보면, 분류하고자 하는 클래스가 k개일 때, k차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다는 내용을 식으로 기재했다.

![https://wikidocs.net/images/page/35476/softmax1_final_final_ver.PNG](https://wikidocs.net/images/page/35476/softmax1_final_final_ver.PNG)

- 샘플 데이터를 1개씩 입력으로 받아 처리한다고 가정해보자. 즉, 배치 크기가 1이다.
- 소프트맥스 함수의 입력

  → 하나의 샘플 데이터는 4개의 독립변수 x를 가지는데, 이는 모델이 4차원 벡터를 입력으로 받음을 의미한다. 하지만 소프트맥스의 함수의 입력으로 사용되는 벡터는 벡터의 차원이 분류하고자 하는 클래스의 개수가 되어야 하므로, 어떤 가중치 연산을 통해 3차원 벡터로 변환되어야 한다.

  ![https://wikidocs.net/images/page/35476/softmaxbetween1and2.PNG](https://wikidocs.net/images/page/35476/softmaxbetween1and2.PNG)

  이는 간단하다. 소프트맥스 함수의 입력 벡터 z의 차원수만큼 결과값이 나오도록 가중치 곱을 진행한다.

  위의 그림에서 화살표는 총 (4 x 3 = 12) 12개이며, 전부 다른 가중치를 가지고, 학습 과정에서 점차적으로 오차를 최소화하는 가중치로 값이 변경된다.

- 오차 계산 방법

  → 소프트맥스 함수의 출력은 분류하고자하는 클래스의 개수만큼 차원을 가지는 벡터로 각 원소는 0과 1사이의 값을 가진다.

  → 이 각각은 특정 클래스가 정답일 확률을 나타낸다.

  → 원-핫 벡터

  ![https://wikidocs.net/images/page/35476/softmax2_final.PNG](https://wikidocs.net/images/page/35476/softmax2_final.PNG)

![https://wikidocs.net/images/page/35476/softmax4_final.PNG](https://wikidocs.net/images/page/35476/softmax4_final.PNG)

- 현재 풀고 있는 샘플 데이터의 실제값이 setosa라면 원-핫 벡터는 [0 1 0]이다. 이 경우, 예측값과 실제값의 오차가 0이 되는 경우는 소프트맥스 함수의 결과가 [0 1 0]이 되는 경우이다.
- 이 두 벡터의 오차를 계산하기 위해서 소프트맥스 회귀는 비용 함수로 크로스 엔트로피 함수를 사용한다.

![https://wikidocs.net/images/page/35476/softmax5_final.PNG](https://wikidocs.net/images/page/35476/softmax5_final.PNG)

- 그리고 앞서 배운 선형회귀나 로지스틱 회귀와 마찬가지로 오차로부터 가중치를 업데이트 한다.

![https://wikidocs.net/images/page/35476/softmax6_final_2ldz1s0.PNG](https://wikidocs.net/images/page/35476/softmax6_final_2ldz1s0.PNG)

- 더 정확히는 선형 회귀나 로지스틱 회귀와 마찬가지로 편향 또한 업데이트의 대상이 되는 매개 변수 이다.
- 입력을 특성(feature)의 수만큼의 차원을 가진 입력 벡터 x라고 하고, 가중치 행렬을 W, 편향을 b라고 했을 때, 소프트맥스 회귀에서 예측값을 구하는 과정을 벡터와 행렬 연산으로 표현하면 아래와 같다.

![https://wikidocs.net/images/page/35476/softmax7.PNG](https://wikidocs.net/images/page/35476/softmax7.PNG)

## 3. 원-핫 벡터의 무작위성

- 정수 인코딩에 원-핫 벡터를 사용하는 이유는 클래스의 특성을 잘 유지하기 위해서다.
- 각 클래스가 순서의 의미를 갖고 있어서 회귀를 통해서 분류문제를 풀 수 있는 경우 [1,2,3...] 식으로 정수 인코딩을 해도 되지만, 일반적인 분류 문제에서는 각 클래스는 순서의 의미를 갖고 있지 않으므로, 각 클래스 간의 오차는 균등한 것이 옳다.
- 정수 인코딩과 달리 원-핫 인코딩은 분류 문제 모든 클래스 간의 관계를 균등하게 분배한다.

$$((1,0,0)-(0,1,0))^{2} = (1-0)^{2} + (0-1)^{2} + (0-0)^{2} = 2\\((1,0,0)-(0,0,1))^{2} = (1-0)^{2} + (0-0)^{2} + (0-1)^{2} = 2$$

- 다르게 표현하면 모든 클래스에 대해서 원-핫 인코딩을 통해 얻은 원-핫 벡터들은 모든 쌍에 대해서 유클리드 거리를 구해도 전부 유클리드 거리가 동일하다.
- 원-핫 벡터는 이처럼 각 클래스의 표현 방법이 무작위성을 가진다는 점을 표현할 수 있다.
- 하지만 이러한 원-핫 벡터의 관계의 무작위성은 때로는 단어의 유사성을 구할 수 없다는 단점으로 언급되기도 한다.

## 4. 비용 함수 (Cost function)

### 1. 크로스 엔트로피 함수

$$cost(W) = -\sum_{j=1}^{k}y_{j}\ log(p_{j})$$

- $c$가 실제값 원-핫 벡터에서 1을 가진 원소의 인덱스라고 한다면, $p_c = 1$은 $\hat{y}$가 $y$를 정확하게 예측한 경우가 된다. 이를 식에 대입해서 보면 $-1log(1) = 0$이 되기 때문에, 결과적으로 $\hat{y}$가 $y$를 정확하게 예측한 경우의 크로스 엔트로피 함수의 값은 0이 된다. 즉 해당 비용을 최소화하는 방향으로 학습을 하면 된다.
- 이제 이를 n개의 전체 데이터에 대한 평균을 구한다고 하면 최종 비용 함수는 다음과 같다.

$$cost(W) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ log(p_{j}^{(i)})$$

### 2. 이진 분류에서의 크로스 엔트로피 함수

- 로지스틱 회귀에서 이용한 이진 분류에 쓰이는 크로스 엔트로피 함수식과는 달라보이지만, 본질적으로는 동일하다.

$$-(\sum_{i=1}^{2}y_{i}\ log\ p_{i})$$

- 소프트맥스 회귀에서는 k의 값이 고정된 값이 아니므로, 2를 k로 변경하면 된다.
- 결과적으로 소프트맥스 회귀의 식과 동일하다.
- 정리하면 소프트맥스 함수의 최종 비용 함수에서 k가 2라고 가정하면 결국 로지스틱 회귀의 비용함수와 같다.

## 5. 인공 신경망 다이어그램

- n개의 특성을 가지고, m개의 클래스를 분류하는 소프트맥스 회귀를 뒤에서 배우게 되는 인공 신경망의 형태로 표현하면 다음과 같다.

![https://wikidocs.net/images/page/35476/softmax_regression_nn.PNG](https://wikidocs.net/images/page/35476/softmax_regression_nn.PNG)

- 소프트맥스 회귀 또한 하나의 인공 신경망으로 볼 수 있으므로, 소프트맥스 회귀로부터 인공 신경망을 이해하는 것은 어렵지 않다.
